data:
  train_data_path: "./data/train.csv"
  test_data_path: "./data/test.csv"

model:
  model_name: "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"
  h_param: 
    max_length: 512
    
train:
  output_dir: "./outputs"
  eval_strategy: "epoch"
  save_strategy: "epoch"
  logging_strategy: "steps"
  logging_steps: 100
  epochs: 3
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 32
  lr: 1e-5
  weight_decay: 0.01
  warmup_ratio: 0.06
  metric_for_best_model: "f1"

lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

sft_config:
  output_dir: "./exaone"
  per_device_batch_size: 2
  gradient_accumulation_steps: 8
  lr: 2e-4
  epochs: 3
  logging_steps: 10
  save_steps: 200

inference:
  inference_model_dir: "./outputs/klue_bert_cls"