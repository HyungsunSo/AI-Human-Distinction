# ğŸ› ï¸ Dev Log & Idea Sketch

## ğŸ“… 2026-01-12: ëª¨ë¸ë§ ì „ëµ ìˆ˜ë¦½ (Phase 1)

### 1. ğŸ¯ í•µì‹¬ ë¬¸ì œ ì •ì˜ (Problem Definition)

* **Label Granularity Mismatch**:
  * **Train Data**: `Full Text` ë‹¨ìœ„ì˜ ë¼ë²¨ (0 or 1). `generated=1`ì¸ ê²½ìš°ì—ë„ ì¼ë¶€ ë¬¸ë‹¨ì€ ì‚¬ëŒ(Human)ì´ ì¼ì„ ê°€ëŠ¥ì„±ì´ ìˆìŒ (Label Noise ì¡´ì¬).
  * **Test Data**: `Paragraph` ë‹¨ìœ„ì˜ í™•ë¥  ì˜ˆì¸¡.
* **Goal**: ë¬¸ë‹¨(Paragraph) ë‹¨ìœ„ì˜ ì •ë°€í•œ AI/Human íŒë³„ ëª¨ë¸ êµ¬ì¶•.

### 2. ğŸ’¡ ì ‘ê·¼ ì „ëµ (Strategy): Synthetic Data & 2-Stage Modeling

#### 2.1. ë°ì´í„°ì…‹ êµ¬ì¶• ì•„ì´ë””ì–´ (Synthetic Dataset Construction)

* **ê°€ì„¤**: Human ë°ì´í„°(`generated=0`)ì˜ ë¬¸ë‹¨ì€ 100% ì‚¬ëŒì´ ì“´ ê²ƒìœ¼ë¯€ë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” "Source"ì´ë‹¤.
* **ìƒì„± ëª¨ë¸(GenAI) í™œìš©**:
  * Source(Human Paragraph)ë¥¼ LLM(e.g., HyperCLOVA X, GPT-4, or Open Source LLM)ì— ì£¼ì….
  * ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í˜• ìƒì„± (AI ë°ì´í„° í™•ë³´):
    1. **Re-writing**: "ì´ ë¬¸ë‹¨ì„ AI ìŠ¤íƒ€ì¼ë¡œ ë‹¤ì‹œ ì¨ì¤˜."
    2. **Summarization**: "ì´ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜."
    3. **Expansion**: "ì´ ë‚´ìš©ì„ ì´ì–´ ì¨ì¤˜."
  * **ê²°ê³¼**: (Human Para, 0) vs (Generated AI Para, 1)ì˜ ì™„ë²½í•œ ë¬¸ë‹¨ ë‹¨ìœ„ ìŒ(Pair) ë°ì´í„°ì…‹ í™•ë³´ ê°€ëŠ¥.

#### 2.2. ëª¨ë¸ êµ¬ì¡° ì•„ì´ë””ì–´ (2nd Order Structure)

* ë‹¨ìˆœ ë¶„ë¥˜ê¸°ë¥¼ ë„˜ì–´ì„  2ë‹¨ê³„ ì ‘ê·¼ë²• ì œì•ˆ:
  * **Stage 1 (Generator/Teacher)**:
    * Human ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³ í’ˆì§ˆì˜ AI ë¬¸ë‹¨ ìƒì„± (Data Augmentation).
  * **Stage 2 (Discriminator/Student)**:
    * ìƒì„±ëœ Synthetic Datasetìœ¼ë¡œ 1ì°¨ ë¬¸ë‹¨ íŒë³„ê¸° í•™ìŠµ (BERT/RoBERTa ë“±).
  * **Stage 3 (Refinement/Pseudo-labeling)**:
    * í•™ìŠµëœ 1ì°¨ íŒë³„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬, **ê¸°ì¡´ Train Data(`generated=1`)ì˜ ë¬¸ë‹¨ë“¤ì„ ê²€ìˆ˜**.
    * ê¸°ì¡´ ë°ì´í„° ì¤‘ "ì§„ì§œ AI ê°™ì€ ë¬¸ë‹¨"ë§Œ í•„í„°ë§(Denoising)í•˜ê±°ë‚˜ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ëª¨ë¸ ì¬í•™ìŠµ (Self-training).

### 3. ğŸ“ To-Do List

- [ ] ë¬¸ë‹¨ ë‹¨ìœ„ ë¶„ë¦¬(Split) ì „ì²˜ë¦¬ ë¡œì§ êµ¬í˜„.
- [ ] ìƒì„± ëª¨ë¸(LLM) ì„ ì • ë° í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ (AI ìŠ¤íƒ€ì¼ ëª¨ë°©).
- [ ] ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸(BERT/RoBERTa) ì„ ì • (`klue/roberta-large` ë“±).

---

## ğŸ“… 2026-01-12: DeepSeek-R1 ë°©ë²•ë¡  ì ìš© ì•„ì´ë””ì–´ (Phase 2)

### ğŸ”¬ DeepSeek-R1 í•µì‹¬ ë°©ë²•ë¡  ìš”ì•½

([ì°¸ê³ ](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms))

1. **Cold Start**: ì†ŒëŸ‰ì˜ ê³ í’ˆì§ˆ ë°ì´í„°ë¡œ SFT
2. **RL (ê°•í™”í•™ìŠµ)**: Rule-based Reward (ì •í™•ë„, í˜•ì‹)ë¡œ ëª¨ë¸ í–‰ë™ ìœ ë„
3. **Rejection Sampling**: ìƒì„±ëœ ë°ì´í„° ì¤‘ í’ˆì§ˆ ì¢‹ì€ ê²ƒë§Œ ì„ ë³„ â†’ Synthetic Dataset êµ¬ì¶•
4. **Distillation**: í° Teacher ëª¨ë¸ì˜ ì§€ì‹ì„ ì‘ì€ Student ëª¨ë¸ì— ì „ì´

### ğŸ’¡ ìš°ë¦¬ ë¬¸ì œì— ì ìš© (ì¶”ìƒì  ë ˆë²¨)

| DeepSeek-R1 ë‹¨ê³„                    | ìš°ë¦¬ ì ìš©                                                                            |
| ----------------------------------- | ------------------------------------------------------------------------------------ |
| **Cold Start (SFT)**          | Human 100% í™•ì‹  ë°ì´í„°(`generated=0`)ì˜ ë¬¸ë‹¨ìœ¼ë¡œ **Synthetic AI ë¬¸ë‹¨ ìƒì„±**  |
| **Rejection Sampling**        | ìƒì„±ëœ AI ë¬¸ë‹¨ ì¤‘**í’ˆì§ˆ ì¢‹ì€ ê²ƒë§Œ í•„í„°ë§** (Reward Model or Rule-based)        |
| **RL with Rule-based Reward** | íŒë³„ê¸°(Discriminator)ì— `ì •í™•ë„ Reward` + `ì¼ê´€ì„± Reward` ì„¤ê³„í•˜ì—¬ Self-Training |
| **Distillation**              | LLM(Teacher)ì´ ìƒì„±í•œ Synthetic Dataë¡œ**ì‘ì€ BERT(Student) í•™ìŠµ**              |

### ğŸ”„ Proposed Pipeline (DeepSeek-Inspired 2-Stage)

```
[Phase 1: Data Generation]
Human ë¬¸ë‹¨ (Label=0)
        â”‚
        â–¼
   LLM (Generator) â”€â”€â”€â”€â”€â”€â”€â”€â”€> AI ìŠ¤íƒ€ì¼ ë¬¸ë‹¨ ìƒì„±
        â”‚
        â–¼
   Rejection Sampling â”€â”€â”€â”€â”€â”€â”€â”€> í’ˆì§ˆ ì¢‹ì€ AI ë¬¸ë‹¨ ì„ ë³„ (Rule or Reward Model)
        â”‚
        â–¼
Synthetic Dataset: (Human Para, 0) vs (AI Para, 1)


[Phase 2: Model Training]
        â”‚
        â–¼
   BERT/RoBERTa (Discriminator) í•™ìŠµ
        â”‚
        â–¼
   ê¸°ì¡´ Train Data (generated=1) Pseudo-labeling / Denoising
        â”‚
        â–¼
   Self-Training / Refinement
```

### í•µì‹¬ í¬ì¸íŠ¸

* **Generatorê°€ Discriminatorë¥¼ ë•ëŠ” êµ¬ì¡°**: ìƒì„± ëª¨ë¸ì´ ë§Œë“¤ì–´ë‚¸ ë°ì´í„°ë¡œ íŒë³„ ëª¨ë¸ì„ í•™ìŠµ.
* **Rule-based Reward**: ìƒì„±ëœ AI ë¬¸ë‹¨ì´ "AI ê°™ì€ì§€" íŒë‹¨í•˜ëŠ” ê·œì¹™ (e.g., perplexity, vocabulary diversity, ë°˜ë³µ íŒ¨í„´ ë“±).
* **Distillation íš¨ê³¼**: LLMì˜ ì§€ì‹(AI ìŠ¤íƒ€ì¼)ì„ ì‘ì€ BERTê°€ í•™ìŠµ.

---

## ğŸ“… 2026-01-12: Feature Analysis ê²°ê³¼ (í†µê³„ì  ì§€ë¬¸)

KoGPT2ë¥¼ ì´ìš©í•´ Human(100ê°œ) vs AI(100ê°œ) ìƒ˜í”Œì˜ í†µê³„ì  íŠ¹ì§•ì„ ë¶„ì„í•¨.

### ğŸ“Š ì£¼ìš” ë°œê²¬ (Key Insights)

![1768191196975](image/devlog/1768191196975.png)


perplexity  mean_entropy  logprob_std  low_prob_ratio  trigram_rep_ratio  
0  194.258610      4.549955     3.549926        0.494118           0.011236
1   52.300789      3.020914     3.616106        0.356863           0.005882
2  113.846155      4.574265     3.174602        0.458824           0.011390
3   88.632998      4.332357     2.948958        0.380392           0.003782
4   76.841650      4.189429     2.833590        0.392157           0.000000

   bigram_rep_ratio  generated  label
0          0.049407          0  Human
1          0.030488          0  Human
2          0.025822          0  Human
3          0.024100          1     AI
4          0.006667          1     AI

1. **Perplexity (ë³µì¡ë„/ì˜ˆì¸¡ë¶ˆê°€ëŠ¥ì„±)**

* **AI (95.6) < Human (126.9)**
* AI í…ìŠ¤íŠ¸ê°€ í›¨ì”¬ **"ë§¤ë„ëŸ½ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•¨"**. Human í…ìŠ¤íŠ¸ëŠ” ì˜ì™¸ì˜ ë‹¨ì–´ ì„ íƒì´ë‚˜ ë…ì°½ì ì¸ í‘œí˜„ìœ¼ë¡œ ì¸í•´ Perplexityê°€ ë†’ìŒ.
* â¡ï¸ **Rule-based Filter 1**: `Perplexity < 110` ì¸ ê²½ìš° "AIìŠ¤ëŸ¬ì›€"ìœ¼ë¡œ íŒë‹¨ ê°€ëŠ¥.

2. **Logprob Consistency (í™•ë¥  ì¼ê´€ì„±)**

   * **AI (3.03) < Human (3.34)** (Logprob Std)
   * AIëŠ” ìƒì„± ì‹œ í™•ë¥  ë¶„í¬ê°€ ë¹„êµì  ì¼ê´€ë¨. ì‚¬ëŒì€ ë¬¸ì¥ë§ˆë‹¤ í™•ì‹ /ë¶ˆí™•ì‹  í¸ì°¨ê°€ í¼.
3. **Repetition (ë°˜ë³µì„±)**

   * **AI (0.023) < Human (0.030)** (Bigram Repetition)
   * ì˜ì™¸ë¡œ **ì‚¬ëŒì´ ë°˜ë³µì„ ë” ë§ì´ í•¨**. (íŠ¹ì • ì£¼ì œ ê°•ì¡°, ê´€ìš©êµ¬ ì‚¬ìš© ë“±).
   * AIëŠ” Diverse decoding(Sampling) ë•ë¶„ì— ì˜¤íˆë ¤ ë°˜ë³µì„ íšŒí”¼í•˜ëŠ” ê²½í–¥ì´ ìˆìŒ.
   * â¡ï¸ "ë°˜ë³µì´ ë§ë‹¤ê³  ë¬´ì¡°ê±´ AIëŠ” ì•„ë‹˜" (ì˜¤íˆë ¤ ê·¸ ë°˜ëŒ€ì¼ ìˆ˜ ìˆìŒ).

### ğŸš€ ì ìš© ì „ëµ: Rejection Sampling Criteria

LLMìœ¼ë¡œ Synthetic Data ìƒì„± í›„, ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ë§Œ **"High-Quality AI Data"**ë¡œ ì±„íƒí•˜ì—¬ í•™ìŠµì— ì‚¬ìš©:

1. **Low Perplexity**: `Perplexity`ê°€ ë‚®ì„ìˆ˜ë¡ (ì˜ˆ: 100 ì´í•˜) AI íŠ¹ì§•ì´ ê°•í•¨.
2. **Low Logprob Std**: í™•ë¥  ë³€ë™ì„±ì´ ë‚®ì€ ìƒ˜í”Œ.

---

## ğŸ“ Feature Extraction Code Analysis (Dimensional Breakdown)

`get_lm_features` í•¨ìˆ˜ ë‚´ë¶€ ë¡œì§ ë° í…ì„œ ì°¨ì›(Shape) ë³€í™” ë¶„ì„.
(ê°€ì •: `batch_size=1`, `seq_len=50`, `vocab_size=51200`)

### 1. Input Processing

```python
inputs = tokenizer(text, return_tensors="pt", ...).to(device)
# inputs['input_ids'] Shape: (1, 50) -> (Batch, Seq_Len)
```

### 2. Model Forward

```python
outputs = model(**inputs, labels=inputs["input_ids"])
logits = outputs.logits[:, :-1, :] 
# Raw Logits: (1, 50, 51200)
# Sliced Logits: (1, 49, 51200) -> (Batch, Seq_Len-1, Vocab_Size)
# (ë§ˆì§€ë§‰ í† í°ì€ ë§ì¶œ ì •ë‹µì´ ì—†ìœ¼ë¯€ë¡œ ì œì™¸)

labels = inputs["input_ids"][:, 1:] 
# Labels: (1, 49) -> (Batch, Seq_Len-1)
# (ì²« í† í°ì€ ì˜ˆì¸¡ ëŒ€ìƒì´ ì•„ë‹ˆë¯€ë¡œ ì œì™¸)
```

### 3. Feature Calculation (Line-by-Line)

#### Perplexity (ë³µì¡ë„)

```python
loss = outputs.loss.item()      # Scalar (e.g., 4.5)
perplexity = np.exp(loss)       # Scalar (e.g., 90.01)
```

* ì „ì²´ Lossë¥¼ ì§€ìˆ˜í™”í•˜ì—¬ "í‰ê·  í—·ê°ˆë¦¼ ì •ë„(Branching Factor)"ë¥¼ ì¸¡ì •.

#### Token-level Probabilities

```python
probs = torch.softmax(logits, dim=-1) 
# Shape: (1, 49, 51200)
# (Vocab ì°¨ì›ì— ëŒ€í•´ í™•ë¥ í•© 1.0ìœ¼ë¡œ ì •ê·œí™”)
```

#### Mean Entropy (ë¶ˆí™•ì‹¤ì„±)

```python
entropy = -torch.sum(probs * torch.log(probs + 1e-9), dim=-1)
# 1. probs * log(probs): (1, 49, 51200)
# 2. Sum over dim=-1: (1, 49) -> ê° í† í°ë³„ ì—”íŠ¸ë¡œí”¼ ê°’
mean_entropy = entropy.mean().item() # Scalar (í‰ê· )
```

#### Logprob of Actual Tokens (ì •ë‹µ í™•ë¥ )

```python
log_probs = torch.log(probs + 1e-9) 
# Shape: (1, 49, 51200)

actual_logprobs = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)
# 1. labels.unsqueeze(-1): (1, 49, 1) -> ì¸ë±ì‹±ì„ ìœ„í•´ ì°¨ì› í™•ì¥
# 2. gather: (1, 49, 51200)ì—ì„œ ì •ë‹µ ì¸ë±ìŠ¤ ìœ„ì¹˜ì˜ ê°’ë§Œ ì¶”ì¶œ -> (1, 49, 1)
# 3. squeeze: (1, 49) -> ë‹¤ì‹œ 2ì°¨ì›ìœ¼ë¡œ ë³µê·€
```

#### Statistics (ì¼ê´€ì„± ë° ì €í™•ë¥  ë¹ˆë„)

```python
logprob_std = actual_logprobs.std().item() 
# Scalar (ë¡œê·¸ í™•ë¥ ë“¤ì˜ í‘œì¤€í¸ì°¨ -> ì¼ê´€ì„± ì§€í‘œ)

low_prob_ratio = (actual_logprobs < -5).float().mean().item()
# 1. (actual_logprobs < -5): (1, 49) Bool Tensor (True/False)
# 2. .float(): (1, 49) Float Tensor (1.0/0.0)
# 3. .mean(): Scalar (ë¹„ìœ¨)
```
