# ğŸ› ï¸ Dev Log & Idea Sketch (2026-01-12)

> **ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2026-01-12 18:06

---

## ğŸ“‹ ëª©ì°¨
1. [ëª¨ë¸ë§ ì „ëµ ìˆ˜ë¦½](#ëª¨ë¸ë§-ì „ëµ-ìˆ˜ë¦½)
2. [DeepSeek-R1 ë°©ë²•ë¡  ì ìš©](#deepseek-r1-ë°©ë²•ë¡ -ì ìš©)
3. [Feature Analysis ê²°ê³¼](#feature-analysis-ê²°ê³¼)
4. [Lexical & Stylistic Analysis](#lexical--stylistic-analysis)
5. [BERT Baseline Pipeline](#bert-baseline-pipeline)
6. [í† í° ê¸¸ì´ ë¶„ì„](#í† í°-ê¸¸ì´-ë¶„ì„)
7. [Synthetic Pair Dataset íŒŒì´í”„ë¼ì¸](#synthetic-pair-dataset-íŒŒì´í”„ë¼ì¸)
8. [ì²« ë²ˆì§¸ ë² ì´ìŠ¤ë¼ì¸ ì œì¶œ ê²°ê³¼](#ì²«-ë²ˆì§¸-ë² ì´ìŠ¤ë¼ì¸-ì œì¶œ-ê²°ê³¼)
9. [í•™ìŠµ ì „ëµ ë¹„êµ ë° ê²°ì •](#í•™ìŠµ-ì „ëµ-ë¹„êµ-ë°-ê²°ì •)
10. [ë°©ë²•ë¡  ì •ë‹¹í™” (Mentor ë°œí‘œìš©)](#ë°©ë²•ë¡ -ì •ë‹¹í™”)
11. [Ablation Study ê³„íš](#ablation-study-ê³„íš)

---

## ëª¨ë¸ë§ ì „ëµ ìˆ˜ë¦½

### ğŸ¯ í•µì‹¬ ë¬¸ì œ ì •ì˜ (Problem Definition)

| ë¬¸ì œ | ì„¤ëª… |
|------|------|
| **Label Granularity Mismatch** | Trainì€ Full Text ë‹¨ìœ„, TestëŠ” Paragraph ë‹¨ìœ„ |
| **Label Noise** | `generated=1`ì—ë„ Human ë¬¸ë‹¨ì´ ì„ì—¬ìˆìŒ |
| **í´ë˜ìŠ¤ ë¶ˆê· í˜•** | AI 8% vs Human 92% |

### ğŸ’¡ ì ‘ê·¼ ì „ëµ: Synthetic Data & 2-Stage Modeling

**ê°€ì„¤**: Human ë°ì´í„°(`generated=0`)ì˜ ë¬¸ë‹¨ì€ 100% ì‹ ë¢° ê°€ëŠ¥í•œ "Source"

**ìƒì„± ì „ëµ**:
1. **Re-writing**: "ì´ ë¬¸ë‹¨ì„ AI ìŠ¤íƒ€ì¼ë¡œ ë‹¤ì‹œ ì¨ì¤˜."
2. **ê²°ê³¼**: (Human Para, 0) vs (Generated AI Para, 1) ìŒ ë°ì´í„°ì…‹ í™•ë³´

---

## DeepSeek-R1 ë°©ë²•ë¡  ì ìš©

| DeepSeek-R1 ë‹¨ê³„ | ìš°ë¦¬ ì ìš© |
|---|---|
| **Cold Start (SFT)** | Human ë¬¸ë‹¨ìœ¼ë¡œ Synthetic AI ë¬¸ë‹¨ ìƒì„± |
| **Rejection Sampling** | ìƒì„±ëœ AI ë¬¸ë‹¨ ì¤‘ í’ˆì§ˆ ì¢‹ì€ ê²ƒë§Œ í•„í„°ë§ |
| **Distillation** | LLM(Teacher)ì´ ìƒì„±í•œ ë°ì´í„°ë¡œ BERT(Student) í•™ìŠµ |

```
[Phase 1: Data Generation]
Human ë¬¸ë‹¨ â†’ LLM (Generator) â†’ AI ìŠ¤íƒ€ì¼ ë¬¸ë‹¨ ìƒì„± â†’ Synthetic Dataset

[Phase 2: Model Training]
BERT/RoBERTa (Discriminator) í•™ìŠµ â†’ Self-Training / Refinement
```

---

## Feature Analysis ê²°ê³¼

KoGPT2ë¥¼ ì´ìš©í•œ Human(100ê°œ) vs AI(100ê°œ) ë¶„ì„

| Feature | AI | Human | í•´ì„ |
|---------|-----|-------|------|
| **Perplexity** | 95.6 | 126.9 | AIê°€ ë” ì˜ˆì¸¡ ê°€ëŠ¥ |
| **Logprob Std** | 3.03 | 3.34 | AIê°€ ë” ì¼ê´€ì  |
| **Bigram Rep** | 0.023 | 0.030 | ì‚¬ëŒì´ ë°˜ë³µ ë” ë§ìŒ |

**Rule-based Filter**: `Perplexity < 110` â†’ AI ê°€ëŠ¥ì„± ë†’ìŒ

---

## Lexical & Stylistic Analysis

| Feature | AI | Human | í•´ì„ |
|---------|-----|-------|------|
| **Formal Endings (-ë‹¤/-ë‹ˆë‹¤)** | 0.298 | 0.007 | AIê°€ ê²©ì‹ì²´ ì„ í˜¸ |
| **Conjunction Density** | 0.93% | 0.68% | AIê°€ ì ‘ì†ì‚¬ ë” ì‚¬ìš© |
| **Tilde (~)** | ë‚®ìŒ | ë†’ìŒ | ì‚¬ëŒì´ ê°ì • í‘œí˜„ |

**Strong Rule**: `Formal End Ratio > 0.1` â†’ AIì¼ í™•ë¥  ê¸‰ìƒìŠ¹

---

## BERT Baseline Pipeline

- **Notebook**: `modeling/bert_baseline.ipynb`
- **Model**: `klue/roberta-base`
- **Input Shape**: `[Batch, 512]`
- **Output Logits**: `[Batch, 2]`

---

## í† í° ê¸¸ì´ ë¶„ì„

| MAX_LEN | Truncation Rate |
|---------|-----------------|
| 512 | **71.25%** |
| 1024 | 31.32% |

**ë¬¸ì œ**: 71% ë°ì´í„°ê°€ ì˜ë¦¼ â†’ **Paragraph Splitting** ì „ëµ ì±„íƒ

---

## Synthetic Pair Dataset íŒŒì´í”„ë¼ì¸

### êµ¬í˜„ ìƒì„¸

| ë‹¨ê³„ | ì„¤ëª… | ê²°ê³¼ |
|------|------|------|
| 1. ë¬¸ë‹¨ ë¶„ë¦¬ | Human ê¸€ â†’ `\n` ê¸°ì¤€ ë¶„ë¦¬ | 1,125,652ê°œ |
| 2. ìƒ˜í”Œë§ | 100ê°œ ë¬¸ì„œ ì„ íƒ | 1,174ê°œ ë¬¸ë‹¨ |
| 3. AI ìƒì„± | Ollama Cloud (`gpt-oss:20b-cloud`) | ì§„í–‰ ì¤‘ |

**ì¶œë ¥ íŒŒì¼**: `synthetic_pairs.csv`

---

## ì²« ë²ˆì§¸ ë² ì´ìŠ¤ë¼ì¸ ì œì¶œ ê²°ê³¼

| í•­ëª© | ê°’ |
|------|-----|
| **ì ìˆ˜** | 0.5127 ~ 0.5161 ROC-AUC |
| **ì˜ë¯¸** | âŒ ëœë¤ ìˆ˜ì¤€ |
| **íŒŒì¼** | `submissions/baseline_v1.csv` |

### ì‹¤íŒ¨ ì›ì¸
1. Label Noise
2. ë‹¨ìœ„ ë¶ˆì¼ì¹˜ (Full Text vs Paragraph)
3. í´ë˜ìŠ¤ ë¶ˆê· í˜•

---

## í•™ìŠµ ì „ëµ ë¹„êµ ë° ê²°ì •

### ROC-AUC ìµœì í™” Loss

| Loss | ì¶”ì²œ |
|------|------|
| CrossEntropy | ê¸°ë³¸ |
| **Focal Loss** | âœ… ì¶”ì²œ |

```python
class FocalLoss(torch.nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, logits, targets):
        ce_loss = F.cross_entropy(logits, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean()
```

### âœ… ê²°ì •: ì „ëµ B (í˜ì–´ â†’ ê· í˜• ì›ë°ì´í„°)

```
[Phase 1] Synthetic Pair + Focal Loss
[Phase 2] ê· í˜• ì›ë°ì´í„° (Human 8k + AI 8k) Fine-tune
```

---

## ë°©ë²•ë¡  ì •ë‹¹í™”

### Q1. ì™œ Synthetic Pair ë°ì´í„°?
- **Label Noise ì œê±°** + **ë¬¸ë‹¨ ë‹¨ìœ„ í•™ìŠµ** ê°€ëŠ¥

### Q2. ì™œ klue/roberta-base?
- í•œêµ­ì–´ íŠ¹í™” + ë¶„ë¥˜ ìµœì í™” + íš¨ìœ¨ì 

### Q3. ì™œ Focal Loss?
- ë¶ˆê· í˜• í•´ê²° + AUC í–¥ìƒ

### Q4. ì™œ 2-Stage í•™ìŠµ?
- Clean â†’ Noisy ìˆœì„œê°€ ì•ˆì •ì 

---

## Ablation Study ê³„íš

| ì‹¤í—˜ ID | Phase 1 | Phase 2 | ê°€ì„¤ |
|---------|---------|---------|------|
| **Exp1** | Synthetic Pair | ê· í˜• ì›ë°ì´í„° | âœ… ìµœê³  |
| **Exp2** | ê· í˜• ì›ë°ì´í„° | Synthetic Pair | ë¹„êµêµ° |
| **Exp3** | Synthetic Pairë§Œ | - | ë°ì´í„° ë¶€ì¡± |
| **Exp4** | ê· í˜• ì›ë°ì´í„°ë§Œ | - | Baseline |

### ê²°ê³¼ ê¸°ë¡
| ì‹¤í—˜ | Train AUC | Val AUC | LB Score |
|------|-----------|---------|----------|
| Exp1 |           |         |          |
| Exp2 |           |         |          |
| Exp3 |           |         |          |
| Exp4 |           |         |          |

---

## ğŸ¯ Training Plan: Strategy B

```mermaid
graph TD
    subgraph Phase1["Phase 1: Synthetic Pre-training"]
        A[synthetic_pairs_cleaned.csv] --> B[1,140 Human ë¬¸ë‹¨]
        A --> C[1,140 AI ë¬¸ë‹¨]
        B --> D["Label = 0 (Human)"]
        C --> E["Label = 1 (AI)"]
        D --> F[2,280ê°œ ìƒ˜í”Œ]
        E --> F
        F --> G["klue/roberta-base í•™ìŠµ"]
        G --> H[checkpoint-phase1.pt]
    end
    
    subgraph Phase2["Phase 2: Balanced Fine-tuning"]
        H --> I[Pre-trained Weights ë¡œë“œ]
        J[train.csv] --> K[Human 8,000ê°œ ìƒ˜í”Œë§]
        J --> L[AI 8,000ê°œ ì „ë¶€]
        K --> M[Label = 0]
        L --> N[Label = 1]
        M --> O[16,000ê°œ ê· í˜• ë°ì´í„°ì…‹]
        N --> O
        I --> P[Fine-tuning]
        O --> P
        P --> Q[final_model.pt]
    end
```

### ğŸ“Œ í•µì‹¬ í¬ì¸íŠ¸

| í•­ëª© | Phase 1 (Pre-training) | Phase 2 (Fine-tuning) |
|------|------------------------|----------------------|
| **ë°ì´í„°** | Synthetic 1,140ìŒ (2,280ê°œ) | ì›ë³¸ ê· í˜• ë°ì´í„° 16,000ê°œ |
| **ëª©ì ** | ê¹¨ë—í•œ íŒ¨í„´ í•™ìŠµ | ì‹¤ì œ ë¶„í¬ ì ì‘ |
| **Epochs** | 3~5 | 2~3 |
| **LR** | 2e-5 | 1e-5 (ë” ë‚®ê²Œ) |
| **Loss** | Focal Loss | Focal Loss |

### ğŸ§© ì½”ë“œ êµ¬ì¡° (ì˜ˆì •)

```python
# modeling/train.py

# --- Phase 1 ---
train_phase1(
    data='synthetic_pairs_cleaned.csv',
    model='klue/roberta-base',
    epochs=3,
    save_to='checkpoint-phase1.pt'
)

# --- Phase 2 ---
train_phase2(
    data='train.csv',  # ê· í˜• ìƒ˜í”Œë§
    model='checkpoint-phase1.pt',
    epochs=2,
    save_to='final_model.pt'
)
