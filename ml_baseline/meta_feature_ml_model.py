"""
Meta Feature ê¸°ë°˜ AI/Human êµ¬ë¶„ ML ëª¨ë¸
- í…ìŠ¤íŠ¸ ì„ë² ë”© ì—†ì´ EDAì—ì„œ ë°œê²¬í•œ ë©”íƒ€ í”¼ì²˜ë§Œ ì‚¬ìš©
- ë¹ ë¥¸ í•™ìŠµê³¼ í•´ì„ ê°€ëŠ¥ì„±ì— ì´ˆì 
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# 1. ë°ì´í„° ë¡œë“œ ë° í”¼ì²˜ ì„ íƒ
# =============================================================================
print("ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
df = pd.read_csv('/Users/youngjinson/ë©‹ì‚¬1/AI-Human-Distinction/open/train_with_all_features.csv')

print(f"ì „ì²´ ë°ì´í„°: {len(df):,}ê°œ")
print(f"ì»¬ëŸ¼: {df.columns.tolist()}")

# EDA ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ë©”íƒ€ í”¼ì²˜ ì„ íƒ
META_FEATURES = [
    # ë°˜ë³µ í‘œí˜„ / ì–´íœ˜ ë‹¤ì–‘ì„±
    'repeat_ratio_mean',      # Human > AI (ì–´íœ˜ ë°˜ë³µ ë§ìŒ)
    'repeat_ratio_p90',
    'ttr_doc',                # Type-Token Ratio
    
    # ê¸°ëŠ¥ì–´ ë°€ë„ (ì¡°ì‚¬, ì–´ë¯¸)
    'particle_per_100char',   # Human > AI
    'ending_per_100char',     # Human > AI  
    'funcword_per_100char',   # Human > AI
    
    # ë¬¸ì„œ/ë¬¸ì¥ ê¸¸ì´ ê´€ë ¨
    'doc_len',                # ë¬¸ì„œ ì „ì²´ ê¸¸ì´
    'sent_len_median',        # ë¬¸ì¥ ì¤‘ì•™ê°’ (Human > AI)
    'sent_len_p90',           # ìƒìœ„ 10% ë¬¸ì¥ ê¸¸ì´
    'sent_len_std',           # í‘œì¤€í¸ì°¨ (Human ë³€ë™ì„± í¼)
    
    # êµ¬ë‘ì  ì‚¬ìš©
    'comma_density',          # 100ìë‹¹ ì‰¼í‘œ (Human > AI)
    
    # ë¬¸ë‹¨ êµ¬ì¡°
    'n_paragraphs',           # ë¬¸ë‹¨ ìˆ˜
]

# clipped ë²„ì „ë„ ì¶”ê°€ (ê·¹ë‹¨ê°’ ì˜í–¥ ì œê±°)
CLIPPED_FEATURES = [
    'repeat_ratio_mean_clipped',
    'ttr_doc_clipped',
    'particle_per_100char_clipped',
    'ending_per_100char_clipped',
    'funcword_per_100char_clipped',
]

# ì‚¬ìš©í•  í”¼ì²˜ ìµœì¢… ë¦¬ìŠ¤íŠ¸
FEATURES_TO_USE = [f for f in META_FEATURES + CLIPPED_FEATURES if f in df.columns]
print(f"\nâœ… ì‚¬ìš©í•  í”¼ì²˜ ({len(FEATURES_TO_USE)}ê°œ): {FEATURES_TO_USE}")

# =============================================================================
# 2. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ - ì¶”ê°€ íŒŒìƒ í”¼ì²˜
# =============================================================================
print("\nğŸ”§ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§...")

# ë¬¸ì¥ ê¸¸ì´ ë³€ë™ê³„ìˆ˜ (CV = std / median)
if 'sent_len_std' in df.columns and 'sent_len_median' in df.columns:
    df['sent_len_cv'] = df['sent_len_std'] / (df['sent_len_median'] + 1e-6)
    FEATURES_TO_USE.append('sent_len_cv')

# ê¸°ëŠ¥ì–´ ë¹„ìœ¨ (ì¡°ì‚¬ vs ì–´ë¯¸)
if 'particle_per_100char' in df.columns and 'ending_per_100char' in df.columns:
    df['particle_ending_ratio'] = df['particle_per_100char'] / (df['ending_per_100char'] + 1e-6)
    FEATURES_TO_USE.append('particle_ending_ratio')

# ë¬¸ì„œ ê¸¸ì´ ëŒ€ë¹„ ë¬¸ë‹¨ ìˆ˜ ë¹„ìœ¨
if 'n_paragraphs' in df.columns and 'doc_len' in df.columns:
    df['para_density'] = df['n_paragraphs'] / (df['doc_len'] + 1e-6) * 1000  # 1000ìë‹¹ ë¬¸ë‹¨ ìˆ˜
    FEATURES_TO_USE.append('para_density')

print(f"âœ… íŒŒìƒ í”¼ì²˜ ì¶”ê°€ í›„ ì´ {len(FEATURES_TO_USE)}ê°œ í”¼ì²˜")

# =============================================================================
# 3. ë°ì´í„° ì¤€ë¹„
# =============================================================================
# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
X = df[FEATURES_TO_USE].copy()
y = df['generated'].copy()

# ë¬´í•œê°’ ì²˜ë¦¬
X = X.replace([np.inf, -np.inf], np.nan)

# ê²°ì¸¡ì¹˜ ì¤‘ì•™ê°’ ëŒ€ì²´
for col in X.columns:
    if X[col].isna().sum() > 0:
        X[col].fillna(X[col].median(), inplace=True)

print(f"\nğŸ“Š í”¼ì²˜ í†µê³„:")
print(X.describe().T)

# Train/Test ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print(f"\nğŸ“Š Train: {len(X_train):,} / Test: {len(X_test):,}")
print(f"í´ë˜ìŠ¤ ë¶„í¬ - Human(0): {(y_train==0).sum():,} / AI(1): {(y_train==1).sum():,}")

# ìŠ¤ì¼€ì¼ë§ (ë¡œì§€ìŠ¤í‹± íšŒê·€ìš©)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# =============================================================================
# 4. ëª¨ë¸ í•™ìŠµ ë° ë¹„êµ
# =============================================================================
print("\n" + "="*60)
print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
print("="*60)

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),
    'HistGradientBoosting': HistGradientBoostingClassifier(max_iter=100, max_depth=6, learning_rate=0.1, random_state=42),
}

results = {}

for name, model in models.items():
    print(f"\nğŸ“Œ {name}")
    
    # ìŠ¤ì¼€ì¼ë§ í•„ìš” ì—¬ë¶€ì— ë”°ë¼ ë°ì´í„° ì„ íƒ
    if name == 'Logistic Regression':
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        y_proba = model.predict_proba(X_test_scaled)[:, 1]
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)[:, 1]
    
    # í‰ê°€
    auc = roc_auc_score(y_test, y_proba)
    results[name] = {
        'model': model,
        'auc': auc,
        'y_pred': y_pred,
        'y_proba': y_proba
    }
    
    print(f"   ROC-AUC: {auc:.4f}")
    print(classification_report(y_test, y_pred, target_names=['Human', 'AI']))

# =============================================================================
# 5. ìµœê³  ëª¨ë¸ ì„ íƒ ë° ìƒì„¸ ë¶„ì„
# =============================================================================
best_model_name = max(results, key=lambda x: results[x]['auc'])
best_result = results[best_model_name]

print("\n" + "="*60)
print(f"ğŸ† ìµœê³  ëª¨ë¸: {best_model_name} (AUC: {best_result['auc']:.4f})")
print("="*60)

# í˜¼ë™ í–‰ë ¬
print("\nğŸ“Š í˜¼ë™ í–‰ë ¬:")
cm = confusion_matrix(y_test, best_result['y_pred'])
print(f"           Predicted")
print(f"           Human  AI")
print(f"Actual Human  {cm[0,0]:>5}  {cm[0,1]:>5}")
print(f"       AI     {cm[1,0]:>5}  {cm[1,1]:>5}")

# í”¼ì²˜ ì¤‘ìš”ë„ (íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì¸ ê²½ìš°)
if best_model_name == 'Random Forest':
    print(f"\nğŸ“ˆ {best_model_name} í”¼ì²˜ ì¤‘ìš”ë„:")
    importance = best_result['model'].feature_importances_
    feat_importance = pd.DataFrame({
        'feature': FEATURES_TO_USE,
        'importance': importance
    }).sort_values('importance', ascending=False)
    
    for idx, row in feat_importance.head(10).iterrows():
        print(f"   {row['feature']:<30} : {row['importance']:.4f}")

elif best_model_name == 'HistGradientBoosting':
    from sklearn.inspection import permutation_importance
    print(f"\nğŸ“ˆ {best_model_name} Permutation í”¼ì²˜ ì¤‘ìš”ë„:")
    perm_importance = permutation_importance(best_result['model'], X_test, y_test, n_repeats=5, random_state=42, n_jobs=-1)
    feat_importance = pd.DataFrame({
        'feature': FEATURES_TO_USE,
        'importance': perm_importance.importances_mean
    }).sort_values('importance', ascending=False)
    
    for idx, row in feat_importance.head(10).iterrows():
        print(f"   {row['feature']:<30} : {row['importance']:.4f}")

# ë¡œì§€ìŠ¤í‹± íšŒê·€ ê³„ìˆ˜ (í•´ì„ìš©)
if 'Logistic Regression' in results:
    print(f"\nğŸ“ˆ Logistic Regression ê³„ìˆ˜ (ì ˆëŒ€ê°’ ê¸°ì¤€ ì •ë ¬):")
    lr_model = results['Logistic Regression']['model']
    coef_df = pd.DataFrame({
        'feature': FEATURES_TO_USE,
        'coef': lr_model.coef_[0]
    })
    coef_df['abs_coef'] = coef_df['coef'].abs()
    coef_df = coef_df.sort_values('abs_coef', ascending=False)
    
    for idx, row in coef_df.head(10).iterrows():
        direction = "â†’ AI" if row['coef'] > 0 else "â†’ Human"
        print(f"   {row['feature']:<30} : {row['coef']:>+.4f} {direction}")

# =============================================================================
# 6. Cross-Validation ìµœì¢… ê²€ì¦
# =============================================================================
print("\n" + "="*60)
print("ğŸ”„ 5-Fold Cross-Validation")
print("="*60)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for name in ['Random Forest', 'HistGradientBoosting']:
    model = models[name]
    cv_scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)
    print(f"{name:<20}: AUC = {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")

print("\nâœ… ì™„ë£Œ!")
