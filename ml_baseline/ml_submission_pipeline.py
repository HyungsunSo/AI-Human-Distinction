"""
ML ê¸°ë°˜ AI/Human êµ¬ë¶„ Submission íŒŒì´í”„ë¼ì¸
===========================================
1. train.csvì—ì„œ ë¬¸ë‹¨ë³„ ë©”íƒ€ í”¼ì²˜ ì¶”ì¶œ (ë¬¸ì„œ â†’ ë¬¸ë‹¨ ë¶„í• )
2. ë¬¸ë‹¨ í”¼ì²˜ë¥¼ ë¬¸ì„œ ë ˆë²¨ë¡œ í’€ë§
3. HistGradientBoosting í•™ìŠµ
4. test.csv ë¬¸ë‹¨ë³„ í”¼ì²˜ ì¶”ì¶œ í›„ ì˜ˆì¸¡
5. sample_submission.csv í˜•ì‹ìœ¼ë¡œ ì €ì¥
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import roc_auc_score
import re
from collections import Counter
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# 1. ë©”íƒ€ í”¼ì²˜ ì¶”ì¶œ í•¨ìˆ˜ë“¤
# =============================================================================

# ê¸°ëŠ¥ì–´ íŒ¨í„´ (ì¡°ì‚¬, ì–´ë¯¸)
PARTICLES = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì™€', 'ê³¼', 'ì˜', 'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜']
ENDINGS = ['ë‹¤', 'ë©°', 'ê³ ', 'ì§€ë§Œ', 'ëŠ”ë°', 'ë©´ì„œ', 'ì§€', 'ë‹ˆ', 'ë¼', 'ì', 'ë ¤ê³ ', 'ë„ë¡', 'ë“¯ì´', 'ì²˜ëŸ¼']

def extract_paragraph_features(text):
    """ë¬¸ë‹¨ì—ì„œ ë©”íƒ€ í”¼ì²˜ ì¶”ì¶œ"""
    if not isinstance(text, str) or len(text.strip()) == 0:
        return {
            'sent_len_median': 0, 'sent_len_p90': 0, 'sent_len_std': 0, 'sent_len_cv': 0,
            'comma_density': 0, 'repeat_ratio': 0, 'ttr': 1,
            'particle_per_100char': 0, 'ending_per_100char': 0, 'funcword_per_100char': 0,
            'para_len': 0, 'n_sentences': 0, 'excl_cnt': 0, 'quest_cnt': 0
        }
    
    text = text.strip()
    para_len = len(text)
    
    # ë¬¸ì¥ ë¶„í• 
    sentences = [s.strip() for s in re.split(r'[.!?ã€‚]\s*', text) if s.strip()]
    if len(sentences) == 0:
        sentences = [text]
    
    sent_lengths = [len(s) for s in sentences]
    
    # ë¬¸ì¥ ê¸¸ì´ í†µê³„
    sent_len_median = np.median(sent_lengths) if sent_lengths else 0
    sent_len_p90 = np.percentile(sent_lengths, 90) if len(sent_lengths) >= 2 else sent_len_median
    sent_len_std = np.std(sent_lengths) if len(sent_lengths) >= 2 else 0
    sent_len_cv = sent_len_std / (sent_len_median + 1e-6)
    
    # ì‰¼í‘œ ë°€ë„ (100ìë‹¹)
    comma_cnt = text.count(',') + text.count('ï¼Œ')
    comma_density = comma_cnt / (para_len / 100) if para_len > 0 else 0
    
    # êµ¬ë‘ì  ì¹´ìš´íŠ¸
    excl_cnt = text.count('!')
    quest_cnt = text.count('?')
    
    # ë°˜ë³µ ë¹„ìœ¨ & TTR (ì–´íœ˜ ë‹¤ì–‘ì„±)
    words = text.split()
    if len(words) > 0:
        unique_words = set(words)
        repeat_ratio = 1 - (len(unique_words) / len(words))
        ttr = len(unique_words) / len(words)
    else:
        repeat_ratio = 0
        ttr = 1
    
    # ê¸°ëŠ¥ì–´ ë°€ë„
    particle_cnt = sum(text.count(p) for p in PARTICLES)
    ending_cnt = sum(text.count(e) for e in ENDINGS)
    
    particle_per_100char = particle_cnt / (para_len / 100) if para_len > 0 else 0
    ending_per_100char = ending_cnt / (para_len / 100) if para_len > 0 else 0
    funcword_per_100char = particle_per_100char + ending_per_100char
    
    return {
        'sent_len_median': sent_len_median,
        'sent_len_p90': sent_len_p90,
        'sent_len_std': sent_len_std,
        'sent_len_cv': sent_len_cv,
        'comma_density': comma_density,
        'repeat_ratio': repeat_ratio,
        'ttr': ttr,
        'particle_per_100char': particle_per_100char,
        'ending_per_100char': ending_per_100char,
        'funcword_per_100char': funcword_per_100char,
        'para_len': para_len,
        'n_sentences': len(sentences),
        'excl_cnt': excl_cnt,
        'quest_cnt': quest_cnt
    }

def pool_paragraph_features(para_features_list):
    """ë¬¸ë‹¨ í”¼ì²˜ë“¤ì„ ë¬¸ì„œ ë ˆë²¨ë¡œ í’€ë§ (mean, std, max, min)"""
    if len(para_features_list) == 0:
        return {}
    
    df = pd.DataFrame(para_features_list)
    pooled = {}
    
    for col in df.columns:
        pooled[f'{col}_mean'] = df[col].mean()
        pooled[f'{col}_std'] = df[col].std() if len(df) > 1 else 0
        pooled[f'{col}_max'] = df[col].max()
        pooled[f'{col}_min'] = df[col].min()
        pooled[f'{col}_median'] = df[col].median()
    
    # ì¶”ê°€ ë¬¸ì„œ ë ˆë²¨ í”¼ì²˜
    pooled['n_paragraphs'] = len(para_features_list)
    pooled['total_len'] = sum(pf['para_len'] for pf in para_features_list)
    
    return pooled

# =============================================================================
# 2. Train ë°ì´í„° ì²˜ë¦¬
# =============================================================================
print("ğŸ“‚ Train ë°ì´í„° ë¡œë”©...")
train_df = pd.read_csv('/Users/youngjinson/ë©‹ì‚¬1/AI-Human-Distinction/open/train.csv')
print(f"Train ë¬¸ì„œ ìˆ˜: {len(train_df):,}")

print("\nğŸ”§ Train í”¼ì²˜ ì¶”ì¶œ (ë¬¸ì„œ â†’ ë¬¸ë‹¨ ë¶„í•  â†’ í’€ë§)...")
train_features = []


for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc="Train í”¼ì²˜ ì¶”ì¶œ"):
    full_text = row['full_text']
    if not isinstance(full_text, str):
        full_text = ""
    
    # ë¬¸ë‹¨ ë¶„í•  (ì¤„ë°”ê¿ˆ ê¸°ì¤€)
    paragraphs = [p.strip() for p in full_text.split('\n') if p.strip()]
    
    # ê° ë¬¸ë‹¨ì—ì„œ í”¼ì²˜ ì¶”ì¶œ
    para_features_list = [extract_paragraph_features(p) for p in paragraphs]
    
    # ë¬¸ì„œ ë ˆë²¨ë¡œ í’€ë§
    if len(para_features_list) > 0:
        doc_features = pool_paragraph_features(para_features_list)
    else:
        doc_features = pool_paragraph_features([extract_paragraph_features("")])
    
    doc_features['generated'] = row['generated']
    train_features.append(doc_features)

train_feat_df = pd.DataFrame(train_features)
print(f"âœ… Train í”¼ì²˜ ì¶”ì¶œ ì™„ë£Œ: {train_feat_df.shape}")

# =============================================================================
# 3. Test ë°ì´í„° ì²˜ë¦¬
# =============================================================================
print("\nğŸ“‚ Test ë°ì´í„° ë¡œë”©...")
test_df = pd.read_csv('/Users/youngjinson/ë©‹ì‚¬1/AI-Human-Distinction/open/test.csv')
print(f"Test ë¬¸ë‹¨ ìˆ˜: {len(test_df):,}")

print("\nğŸ”§ Test í”¼ì²˜ ì¶”ì¶œ (ë¬¸ë‹¨ë³„)...")
test_features = []

for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc="Test í”¼ì²˜ ì¶”ì¶œ"):
    para_text = row['paragraph_text']
    features = extract_paragraph_features(para_text)
    features['ID'] = row['ID']
    features['title'] = row['title']
    features['paragraph_index'] = row['paragraph_index']
    test_features.append(features)

test_feat_df = pd.DataFrame(test_features)
print(f"âœ… Test í”¼ì²˜ ì¶”ì¶œ ì™„ë£Œ: {test_feat_df.shape}")

# =============================================================================
# 4. í•™ìŠµ ë° ì˜ˆì¸¡
# =============================================================================
print("\n" + "="*60)
print("ğŸš€ ëª¨ë¸ í•™ìŠµ")
print("="*60)

# í”¼ì²˜ ì»¬ëŸ¼ ì„ íƒ (ìˆ«ì ì»¬ëŸ¼ë§Œ)
feature_cols = [c for c in train_feat_df.columns if c != 'generated' and train_feat_df[c].dtype in ['float64', 'int64']]
print(f"í”¼ì²˜ ìˆ˜: {len(feature_cols)}")

X_train = train_feat_df[feature_cols].copy()
y_train = train_feat_df['generated'].copy()

# ë¬´í•œê°’/ê²°ì¸¡ì¹˜ ì²˜ë¦¬
X_train = X_train.replace([np.inf, -np.inf], np.nan)
X_train = X_train.fillna(0)

# 5-Fold CVë¡œ í•™ìŠµ
print("\nğŸ“Š 5-Fold Cross Validation...")
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = []
oof_preds = np.zeros(len(X_train))

for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):
    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
    
    model = HistGradientBoostingClassifier(
        max_iter=200, 
        max_depth=8, 
        learning_rate=0.05, 
        random_state=42
    )
    model.fit(X_tr, y_tr)
    
    val_proba = model.predict_proba(X_val)[:, 1]
    oof_preds[val_idx] = val_proba
    
    auc = roc_auc_score(y_val, val_proba)
    cv_scores.append(auc)
    print(f"  Fold {fold+1}: AUC = {auc:.4f}")

print(f"\nğŸ“Š CV í‰ê·  AUC: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores)*2:.4f})")

# ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
print("\nğŸ¯ ìµœì¢… ëª¨ë¸ í•™ìŠµ...")
final_model = HistGradientBoostingClassifier(
    max_iter=200, 
    max_depth=8, 
    learning_rate=0.05, 
    random_state=42
)
final_model.fit(X_train, y_train)

# =============================================================================
# 5. Test ì˜ˆì¸¡ (ë¬¸ë‹¨ë³„ â†’ ë¬¸ì„œ í’€ë§ í•„ìš” ì—†ì´ ë¬¸ë‹¨ë³„ ì˜ˆì¸¡)
# =============================================================================
print("\nğŸ”® Test ì˜ˆì¸¡...")

# Test í”¼ì²˜ ì¤€ë¹„
test_feature_cols = [c for c in feature_cols if c in test_feat_df.columns]
missing_cols = [c for c in feature_cols if c not in test_feat_df.columns]

# í’€ë§ í”¼ì²˜ (_mean, _std ë“±)ëŠ” testì— ì—†ìœ¼ë¯€ë¡œ, ë¬¸ë‹¨ ë‹¨ì¼ í”¼ì²˜ ì‚¬ìš©
# ë¬¸ë‹¨ í”¼ì²˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (mean ì»¬ëŸ¼ì€ í•´ë‹¹ ë¬¸ë‹¨ì˜ ê°’ìœ¼ë¡œ ëŒ€ì²´)
X_test = pd.DataFrame()

# ê¸°ë³¸ ë¬¸ë‹¨ í”¼ì²˜ë“¤
basic_features = ['sent_len_median', 'sent_len_p90', 'sent_len_std', 'sent_len_cv',
                  'comma_density', 'repeat_ratio', 'ttr', 
                  'particle_per_100char', 'ending_per_100char', 'funcword_per_100char',
                  'para_len', 'n_sentences', 'excl_cnt', 'quest_cnt']

for col in feature_cols:
    if col in test_feat_df.columns:
        X_test[col] = test_feat_df[col]
    else:
        # í’€ë§ í”¼ì²˜ì˜ ê²½ìš° ê¸°ë³¸ í”¼ì²˜ì—ì„œ ë§¤í•‘
        base_col = col.replace('_mean', '').replace('_std', '').replace('_max', '').replace('_min', '').replace('_median', '')
        if base_col in test_feat_df.columns:
            if '_std' in col:
                X_test[col] = 0  # ë‹¨ì¼ ë¬¸ë‹¨ì´ë¯€ë¡œ std=0
            elif '_mean' in col or '_median' in col or '_max' in col or '_min' in col:
                X_test[col] = test_feat_df[base_col]
        elif col == 'n_paragraphs':
            X_test[col] = 1  # ë¬¸ë‹¨ ë‹¨ìœ„ì´ë¯€ë¡œ 1
        elif col == 'total_len':
            X_test[col] = test_feat_df['para_len']
        else:
            X_test[col] = 0

X_test = X_test.replace([np.inf, -np.inf], np.nan)
X_test = X_test.fillna(0)

# ì˜ˆì¸¡
test_proba = final_model.predict_proba(X_test)[:, 1]

# =============================================================================
# 6. Submission íŒŒì¼ ìƒì„±
# =============================================================================
print("\nğŸ“ Submission íŒŒì¼ ìƒì„±...")

submission = pd.DataFrame({
    'ID': test_feat_df['ID'],
    'generated': test_proba  # í™•ë¥ ê°’ìœ¼ë¡œ ì¶œë ¥
})

# ì €ì¥
output_path = '/Users/youngjinson/ë©‹ì‚¬1/AI-Human-Distinction/ml_baseline/submission_ml_baseline.csv'
submission.to_csv(output_path, index=False)

print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {output_path}")
print(f"   í‰ê·  AI í™•ë¥ : {test_proba.mean():.4f}")
print(f"   í™•ë¥  ë¶„í¬: min={test_proba.min():.4f}, max={test_proba.max():.4f}")

# ìƒ˜í”Œ í™•ì¸
print("\nğŸ“‹ ì˜ˆì¸¡ ìƒ˜í”Œ:")
print(submission.head(10))
