"""
MIL (Multiple Instance Learning) ê¸°ë°˜ AI/Human êµ¬ë¶„ Submission íŒŒì´í”„ë¼ì¸
========================================================================
1. Stage 1 (Paragraph Training): 
   - train.csvë¥¼ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìª¼ê°œì–´ í•™ìŠµ (Label = ë¬¸ì„œ Label, Noisy Labeling)
2. Stage 2 (Scoring & Pooling): 
   - í•™ìŠµëœ ë¬¸ë‹¨ ëª¨ë¸ë¡œ train.csv ë‚´ ëª¨ë“  ë¬¸ë‹¨ ì ìˆ˜ ì‚°ì¶œ
   - ë¬¸ì„œë³„ë¡œ ë¬¸ë‹¨ ì ìˆ˜ë“¤ì˜ í†µê³„ì¹˜(Max, Mean, Std, Top-K)ë¥¼ í”¼ì²˜ë¡œ ìƒì„±
3. Stage 3 (Meta-Classification):
   - ë¬¸ì„œë³„ í’€ë§ ì ìˆ˜ë¥¼ í”¼ì²˜ë¡œ í•˜ì—¬ ìµœì¢… ë¬¸ì„œ ë¶„ë¥˜ê¸° í•™ìŠµ
4. Inference:
   - test.csv ë¬¸ë‹¨ë³„ ì ìˆ˜ ì‚°ì¶œ
   - title(ë¬¸ì„œ)ë³„ë¡œ ë¬¶ì–´ Meta-Modelì„ í†µí•œ ì ìˆ˜ ë³´ì • (Optional but helpful)
   - ìµœì¢… Submission ìƒì„± (í™•ë¥ ê°’)
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, GroupKFold
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import roc_auc_score
import re
from tqdm import tqdm
import warnings
import os

warnings.filterwarnings('ignore')

# ê¸°ë³¸ ê²½ë¡œ
BASE_DIR = '/Users/youngjinson/ë©‹ì‚¬1/AI-Human-Distinction'
OPEN_DIR = os.path.join(BASE_DIR, 'open')
OUTPUT_DIR = os.path.join(BASE_DIR, 'ml_baseline')

# ê¸°ëŠ¥ì–´ íŒ¨í„´ (ì¡°ì‚¬, ì–´ë¯¸)
PARTICLES = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì™€', 'ê³¼', 'ì˜', 'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜']
ENDINGS = ['ë‹¤', 'ë©°', 'ê³ ', 'ì§€ë§Œ', 'ëŠ”ë°', 'ë©´ì„œ', 'ì§€', 'ë‹ˆ', 'ë¼', 'ì', 'ë ¤ê³ ', 'ë„ë¡', 'ë“¯ì´', 'ì²˜ëŸ¼']

# =============================================================================
# 1. í”¼ì²˜ ì¶”ì¶œ ì—”ì§„
# =============================================================================

def extract_features(text):
    """ë‹¨ì¼ í…ìŠ¤íŠ¸(ë¬¸ë‹¨)ì—ì„œ ë©”íƒ€ í”¼ì²˜ ì¶”ì¶œ"""
    if not isinstance(text, str) or len(text.strip()) == 0:
        return {
            'sent_len_median': 0, 'sent_len_std': 0, 'comma_density': 0, 
            'repeat_ratio': 0, 'ttr': 1, 'particle_density': 0, 
            'ending_density': 0, 'text_len': 0, 'n_words': 0
        }
    
    text = text.strip()
    text_len = len(text)
    words = text.split()
    n_words = len(words)
    
    # ë¬¸ì¥ ë¶„í•  ë° ê¸¸ì´
    sentences = [s.strip() for s in re.split(r'[.!?ã€‚]\s*', text) if s.strip()]
    sent_lengths = [len(s) for s in sentences] if sentences else [0]
    
    # ì–´íœ˜ ë‹¤ì–‘ì„±
    unique_words = set(words)
    repeat_ratio = 1 - (len(unique_words) / n_words) if n_words > 0 else 0
    ttr = len(unique_words) / n_words if n_words > 0 else 1
    
    # ë°€ë„ í”¼ì²˜
    comma_cnt = text.count(',') + text.count('ï¼Œ')
    particle_cnt = sum(text.count(p) for p in PARTICLES)
    ending_cnt = sum(text.count(e) for e in ENDINGS)
    
    norm = text_len / 100 if text_len > 0 else 1
    
    return {
        'sent_len_median': np.median(sent_lengths),
        'sent_len_std': np.std(sent_lengths) if len(sent_lengths) > 1 else 0,
        'comma_density': comma_cnt / norm,
        'repeat_ratio': repeat_ratio,
        'ttr': ttr,
        'particle_density': particle_cnt / norm,
        'ending_density': ending_cnt / norm,
        'text_len': text_len,
        'n_words': n_words
    }

# =============================================================================
# 2. ë°ì´í„° ì¤€ë¹„
# =============================================================================

print("ğŸ“‚ ë°ì´í„° ë¡œë”© ë° ë¬¸ë‹¨ ë¶„í• ...")
train_df = pd.read_csv(os.path.join(OPEN_DIR, 'train.csv'))
test_df = pd.read_csv(os.path.join(OPEN_DIR, 'test.csv'))

# Train ë¬¸ë‹¨ ë¶„ë¦¬
train_paras = []
for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc="Train Paragraph Parsing"):
    full_text = str(row['full_text'])
    paras = [p.strip() for p in full_text.split('\n') if p.strip()]
    for i, p in enumerate(paras):
        feat = extract_features(p)
        feat['doc_idx'] = idx
        feat['generated'] = row['generated']
        train_paras.append(feat)

train_para_df = pd.DataFrame(train_paras)

# Test í”¼ì²˜ ì¶”ì¶œ
test_paras = []
for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc="Test Feature Extraction"):
    feat = extract_features(row['paragraph_text'])
    feat['ID'] = row['ID']
    feat['title'] = row['title']
    test_paras.append(feat)

test_para_df = pd.DataFrame(test_paras)

# =============================================================================
# 3. Stage 1: Paragraph Model Training (Noisy Label)
# =============================================================================

print("\nğŸš€ Stage 1: Paragraph-level Training...")
X_para = train_para_df.drop(['doc_idx', 'generated'], axis=1)
y_para = train_para_df['generated']

# GroupKFold (ë™ì¼ ë¬¸ì„œì˜ ë¬¸ë‹¨ì´ ì„ì´ì§€ ì•Šë„ë¡)
gkf = GroupKFold(n_splits=5)
para_model = HistGradientBoostingClassifier(max_iter=300, random_state=42)

oof_para_preds = np.zeros(len(train_para_df))
for train_idx, val_idx in gkf.split(X_para, y_para, groups=train_para_df['doc_idx']):
    X_tr, X_val = X_para.iloc[train_idx], X_para.iloc[val_idx]
    y_tr, y_val = y_para.iloc[train_idx], y_para.iloc[val_idx]
    para_model.fit(X_tr, y_tr)
    oof_para_preds[val_idx] = para_model.predict_proba(X_val)[:, 1]

print(f"âœ… Para-level OOF AUC: {roc_auc_score(y_para, oof_para_preds):.4f}")

# Re-train on full paragraph data
para_model.fit(X_para, y_para)

# =============================================================================
# 4. Stage 2 & 3: Pooling & Meta-Model
# =============================================================================

print("\nğŸš€ Stage 2 & 3: Pooling & Meta-Model Training...")
train_para_df['para_score'] = oof_para_preds

# ë¬¸ì„œë³„ ì ìˆ˜ í’€ë§
doc_meta_feats = train_para_df.groupby('doc_idx')['para_score'].agg([
    ('max_score', 'max'),
    ('mean_score', 'mean'),
    ('std_score', 'std'),
    ('q75_score', lambda x: np.percentile(x, 75) if len(x)>0 else 0),
    ('min_score', 'min')
]).fillna(0)

doc_meta_feats['actual_label'] = train_df['generated']

# Meta-Model í•™ìŠµ (ë¬¸ì„œ ë ˆë²¨)
X_meta = doc_meta_feats.drop('actual_label', axis=1)
y_meta = doc_meta_feats['actual_label']

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
meta_model = HistGradientBoostingClassifier(max_iter=100, random_state=42)

meta_cv_scores = []
for train_idx, val_idx in skf.split(X_meta, y_meta):
    X_tr, X_val = X_meta.iloc[train_idx], X_meta.iloc[val_idx]
    y_tr, y_val = y_meta.iloc[train_idx], y_meta.iloc[val_idx]
    meta_model.fit(X_tr, y_tr)
    val_probs = meta_model.predict_proba(X_val)[:, 1]
    meta_cv_scores.append(roc_auc_score(y_val, val_probs))

print(f"âœ… Doc-level Meta AUC: {np.mean(meta_cv_scores):.4f}")

# Re-train meta model
meta_model.fit(X_meta, y_meta)

# =============================================================================
# 5. Inference & Score Refinement
# =============================================================================

print("\nğŸ”® Inference & Final Refinement...")
X_test_para = test_para_df.drop(['ID', 'title'], axis=1)
test_para_df['raw_score'] = para_model.predict_proba(X_test_para)[:, 1]

# Titleë³„(ë¬¸ì„œë³„) í’€ë§ ë° Meta-Model ì ìš©
test_doc_groups = test_para_df.groupby('title')['raw_score'].agg([
    ('max_score', 'max'),
    ('mean_score', 'mean'),
    ('std_score', 'std'),
    ('q75_score', lambda x: np.percentile(x, 75) if len(x)>0 else 0),
    ('min_score', 'min')
]).fillna(0)

test_doc_groups['doc_refine_score'] = meta_model.predict_proba(test_doc_groups)[:, 1]

# ë¬¸ë‹¨ ì ìˆ˜ ë³´ì •: 
# (ë¬¸ë‹¨ ìì²´ ì ìˆ˜) ì™€ (í•´ë‹¹ ë¬¸ë‹¨ì´ ì†í•œ ë¬¸ì„œì˜ ì „ì²´ ì ìˆ˜)ë¥¼ ì•™ìƒë¸”
test_para_df = test_para_df.merge(test_doc_groups[['doc_refine_score']], on='title', how='left')

# ìµœì¢… ì ìˆ˜: ë¬¸ë‹¨ ì ìˆ˜ì™€ ë¬¸ì„œ ì ìˆ˜ì˜ ê²°í•©
# ë¬¸ë‹¨ ì ìˆ˜ê°€ ë†’ìœ¼ë©´ì„œ ì†í•œ ë¬¸ì„œë„ AIì¼ í™•ë¥ ì´ ë†’ì„ ë•Œ ì‹œë„ˆì§€
test_para_df['final_score'] = (test_para_df['raw_score'] * 0.7) + (test_para_df['doc_refine_score'] * 0.3)

# =============================================================================
# 6. Submission ìƒì„±
# =============================================================================

submission = pd.DataFrame({
    'ID': test_para_df['ID'],
    'generated': test_para_df['final_score']
})

output_path = os.path.join(OUTPUT_DIR, 'submission_mil_refined.csv')
submission.to_csv(output_path, index=False)

print(f"\nâœ… ì™„ë£Œ! Submission ì €ì¥ë¨: {output_path}")
print(f"ğŸ“Š ìµœì¢… í™•ë¥  ë¶„í¬: {submission['generated'].describe()}")
