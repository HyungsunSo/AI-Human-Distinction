"""
MIL (Multiple Instance Learning) v2 - Iterative Refinement
==========================================================
1. EDA ê¸°ë°˜ ì •ë°€ í”¼ì²˜ ì¶”ì¶œ
2. Stage 1 (Initial Paragraph Model): ë¬¸ì„œ ë¼ë²¨ë¡œ í•™ìŠµ
3. Label Cleaning (Iterative): 
   - AI ë¬¸ì„œ ë‚´ì—ì„œ ì ìˆ˜ê°€ ë‚®ì€(Human-like) ë¬¸ë‹¨ë“¤ì˜ ë¼ë²¨ì„ 0ìœ¼ë¡œ ë³´ì •
4. Stage 2 (Clean Paragraph Model): ì •ì œëœ ë¼ë²¨ë¡œ ì¬í•™ìŠµ
5. Meta-Model (Pooling): 
   - ë¬¸ì„œë³„ í†µê³„ì¹˜ í’€ë§ í›„ ìµœì¢… ë¬¸ì„œ ì ìˆ˜ ì‚°ì¶œ
6. Inference: 
   - ë¬¸ë‹¨ ì ìˆ˜ì™€ ë¬¸ì„œ ì ìˆ˜ ê²°í•© (ìˆœìœ„ ë³´ì¡´í˜• ì•™ìƒë¸”)
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, GroupKFold
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import roc_auc_score
import re
from tqdm import tqdm
import warnings
import os

warnings.filterwarnings('ignore')

# ê¸°ë³¸ ê²½ë¡œ
BASE_DIR = '/Users/youngjinson/ë©‹ì‚¬1/AI-Human-Distinction'
OPEN_DIR = os.path.join(BASE_DIR, 'open')
OUTPUT_DIR = os.path.join(BASE_DIR, 'ml_baseline')

# ê¸°ëŠ¥ì–´ íŒ¨í„´ (ì¡°ì‚¬, ì–´ë¯¸) - EDA ê¸°ë°˜
PARTICLES = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì™€', 'ê³¼', 'ì˜', 'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜']
ENDINGS = ['ë‹¤', 'ë©°', 'ê³ ', 'ì§€ë§Œ', 'ëŠ”ë°', 'ë©´ì„œ', 'ì§€', 'ë‹ˆ', 'ë¼', 'ì', 'ë ¤ê³ ', 'ë„ë¡', 'ë“¯ì´', 'ì²˜ëŸ¼']

def extract_upgraded_features(text):
    """EDA ê¸°ë°˜ ì •ë°€ ë©”íƒ€ í”¼ì²˜ ì¶”ì¶œ"""
    if not isinstance(text, str) or len(text.strip()) == 0:
        return {
            'sent_len_median': 0, 'sent_len_p90': 0, 'sent_len_std': 0, 'sent_len_cv': 0,
            'comma_density': 0, 'particle_density': 0, 'ending_density': 0,
            'repeat_ratio': 0, 'ttr': 1, 'text_len': 0, 'n_words': 0
        }
    
    text = text.strip()
    text_len = len(text)
    words = text.split()
    n_words = len(words)
    
    # ë¬¸ì¥ ë¶„í•  ë° í†µê³„
    sentences = [s.strip() for s in re.split(r'[.!?ã€‚]\s*', text) if s.strip()]
    sent_lengths = [len(s) for s in sentences] if sentences else [0]
    
    # ì–´íœ˜ ë‹¤ì–‘ì„±
    unique_words = set(words)
    repeat_ratio = 1 - (len(unique_words) / n_words) if n_words > 0 else 0
    ttr = len(unique_words) / n_words if n_words > 0 else 1
    
    # ì •ê·œí™” ê¸°ì¤€ (100ìë‹¹)
    norm = text_len / 100 if text_len > 0 else 1
    
    # êµ¬ë‘ì  ë° ê¸°ëŠ¥ì–´
    comma_cnt = text.count(',') + text.count('ï¼Œ')
    particle_cnt = sum(text.count(p) for p in PARTICLES)
    ending_cnt = sum(text.count(e) for e in ENDINGS)
    
    # í”¼ì²˜ ì…‹
    feats = {
        'sent_len_median': np.median(sent_lengths),
        'sent_len_p90': np.percentile(sent_lengths, 90) if len(sent_lengths) >= 2 else np.median(sent_lengths),
        'sent_len_std': np.std(sent_lengths) if len(sent_lengths) > 1 else 0,
        'comma_density': comma_cnt / norm,
        'particle_density': particle_cnt / norm,
        'ending_density': ending_cnt / norm,
        'repeat_ratio': repeat_ratio,
        'ttr': ttr,
        'text_len': text_len,
        'n_words': n_words,
        'avg_word_len': text_len / n_words if n_words > 0 else 0
    }
    feats['sent_len_cv'] = feats['sent_len_std'] / (feats['sent_len_median'] + 1e-6)
    
    return feats

# =============================================================================
# 1. ë°ì´í„° ì¤€ë¹„
# =============================================================================
print("ğŸ“‚ [1/6] ë°ì´í„° ë¡œë”© ë° í”¼ì²˜ ì¶”ì¶œ...")
train_df = pd.read_csv(os.path.join(OPEN_DIR, 'train.csv'))
test_df = pd.read_csv(os.path.join(OPEN_DIR, 'test.csv'))

# Train ë¬¸ë‹¨ ë¶„ë¦¬ & í”¼ì²˜ ì¶”ì¶œ
train_paras = []
for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc="Train parsing"):
    full_text = str(row['full_text'])
    paras = [p.strip() for p in full_text.split('\n') if p.strip()]
    for i, p in enumerate(paras):
        f = extract_upgraded_features(p)
        f.update({'doc_idx': idx, 'p_idx': i, 'generated': row['generated']})
        train_paras.append(f)
train_para_df = pd.DataFrame(train_paras)

# Test í”¼ì²˜ ì¶”ì¶œ
test_paras = []
for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc="Test parsing"):
    f = extract_upgraded_features(row['paragraph_text'])
    f.update({'ID': row['ID'], 'title': row['title'], 'p_idx': row['paragraph_index']})
    test_paras.append(f)
test_para_df = pd.DataFrame(test_paras)

# =============================================================================
# 2. Stage 1: Initial Para Model (Noisy Label)
# =============================================================================
print("\nğŸš€ [2/6] Stage 1 í•™ìŠµ (Noisy Label)...")
features = [c for c in train_para_df.columns if c not in ['doc_idx', 'p_idx', 'generated']]
X_para = train_para_df[features]
y_para = train_para_df['generated']

gkf = GroupKFold(n_splits=5)
para_model = HistGradientBoostingClassifier(max_iter=300, random_state=42)

oof_para_scores = np.zeros(len(train_para_df))
for tr_idx, val_idx in gkf.split(X_para, y_para, groups=train_para_df['doc_idx']):
    para_model.fit(X_para.iloc[tr_idx], y_para.iloc[tr_idx])
    oof_para_scores[val_idx] = para_model.predict_proba(X_para.iloc[val_idx])[:, 1]

# =============================================================================
# 3. Step 2: Label Cleaning (Iterative)
# =============================================================================
print("\nğŸ§¹ [3/6] Label Cleaning (AI ë¬¸ì„œ ë‚´ Human ë¬¸ë‹¨ ì‹ë³„)...")
train_para_df['initial_score'] = oof_para_scores

# AI ë¬¸ì„œ(generated=1)ì´ë©´ì„œ ì ìˆ˜ê°€ ë‚®ì€ ë¬¸ë‹¨ì€ ì‚¬ì‹¤ Humanì¼ í™•ë¥ ì´ ë†’ìŒ
# ì„ê³„ê°’: 0.2 (í•˜ìœ„ ì ìˆ˜ëŠ” Humanìœ¼ë¡œ ê°„ì£¼)
clean_y = train_para_df['generated'].copy()
# AI ë¬¸ì„œ ë‚´ì—ì„œ ì ìˆ˜ê°€ ê·¹íˆ ë‚®ì€ ë¬¸ë‹¨ë“¤ í•„í„°ë§
noise_mask = (train_para_df['generated'] == 1) & (train_para_df['initial_score'] < 0.2)
clean_y[noise_mask] = 0
print(f"   - ì •ì œëœ ë¬¸ë‹¨ ìˆ˜: {noise_mask.sum():,} (AI -> Human ë³´ì •)")

# =============================================================================
# 4. Stage 3: Cleaned Para Model Re-train
# =============================================================================
print("\nğŸš€ [4/6] Stage 3 í•™ìŠµ (Cleaned Label)...")
para_model_clean = HistGradientBoostingClassifier(max_iter=300, random_state=42)
para_model_clean.fit(X_para, clean_y)

# =============================================================================
# 5. Stage 4: Meta-Model & Pooling
# =============================================================================
print("\nğŸš€ [5/6] Meta-Model í•™ìŠµ (Pooling)...")
train_para_df['clean_score'] = para_model_clean.predict_proba(X_para)[:, 1]

doc_meta = train_para_df.groupby('doc_idx')['clean_score'].agg([
    ('max_val', 'max'), ('mean_val', 'mean'), ('q90_val', lambda x: np.percentile(x, 90)), ('std_val', 'std')
]).fillna(0)

doc_meta['actual_label'] = train_df['generated']
meta_model = HistGradientBoostingClassifier(max_iter=100, random_state=42)
meta_model.fit(doc_meta.drop('actual_label', axis=1), doc_meta['actual_label'])

# =============================================================================
# 6. Inference & Submission
# =============================================================================
print("\nğŸ”® [6/6] Inference ë° ìµœì¢… Submission ìƒì„±...")
X_test = test_para_df[features]
test_para_df['raw_score'] = para_model_clean.predict_proba(X_test)[:, 1]

# Title(ë¬¸ì„œ) ë ˆë²¨ë¡œ ë¬¶ì–´ì„œ Meta ì ìˆ˜ ì‚°ì¶œ
test_doc_meta = test_para_df.groupby('title')['raw_score'].agg([
    ('max_val', 'max'), ('mean_val', 'mean'), ('q90_val', lambda x: np.percentile(x, 90)), ('std_val', 'std')
]).fillna(0)

test_doc_meta['doc_score'] = meta_model.predict_proba(test_doc_meta)[:, 1]

# ìµœì¢… ì ìˆ˜ ì‚°ì¶œ (ë¬¸ë‹¨ ì ìˆ˜ ìœ„ì£¼, ë¬¸ì„œ ì ìˆ˜ ë³´ì¡°)
# *ì¤‘ì˜ ì‚¬í•­*: ë¬¸ë‹¨ IDê°€ íƒ€ê²Ÿì´ë¯€ë¡œ ë¬¸ì„œ ì ìˆ˜ê°€ ë„ˆë¬´ ê° ë¬¸ë‹¨ì„ ì§€ë°°í•˜ë©´ ì•ˆ ë¨.
test_final = test_para_df.merge(test_doc_meta[['doc_score']], on='title', how='left')
# ë¬¸ë‹¨ ì ìˆ˜ì™€ ë¬¸ì„œ ì ìˆ˜ì˜ ê³±/ê°€ì¤‘ì¹˜ í•© (ìˆœìœ„ ë³´ì¡´)
test_final['final_prob'] = (test_final['raw_score'] * 0.8) + (test_final['doc_score'] * 0.2)

submission = pd.DataFrame({
    'ID': test_para_df['ID'],
    'generated': test_final['final_prob']
})

out_path = os.path.join(OUTPUT_DIR, 'submission_mil_v2_iterative.csv')
submission.to_csv(out_path, index=False)
print(f"\nâœ… ì™„ë£Œ! {out_path}")
print(f"ğŸ“Š ë¶„í¬ ìš”ì•½:\n{submission['generated'].describe()}")
